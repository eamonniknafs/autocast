{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Fine-Tuned GPT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "autocast_questions = json.load(open('../../autocast_questions.json')) # from the Autocast dataset\n",
    "test_questions = json.load(open('../../autocast_competition_test_set.json'))\n",
    "test_ids = [q['id'] for q in test_questions]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "### Get performance on the Autocast train set\n",
    "\n",
    "Note that the Autocast dataset contains questions in the competition test set. Those should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(probabilities, answer_probabilities):\n",
    "    return ((probabilities - answer_probabilities) ** 2).sum() / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuned_gpt(q):\n",
    "    # query the fine-tuned GPT-3 model\n",
    "    openai.api_key = \"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\"\n",
    "    if q['qtype'] == 'mc':\n",
    "        choices = \"\"\n",
    "        for i, choice in enumerate(q['choices']):\n",
    "            ## first question is A, second is B, etc.\n",
    "            choices += chr(ord('A') + i) + \") \" + choice + \"\\n\"\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"ada:ft-codewise-2023-03-10-04-22-36\",\n",
    "            prompt=q['background'] + \"\\n\\nDate of event: \" + q['publish_time'] + \"\\n\\nWhat is the answer to this question:\\n\" + q['question'] + \"\\n\\nDate of question: \" + q['publish_time'] + \"\\n\\nChoices: \" + choices + \"\\n\\n###\\n\\n\",\n",
    "            temperature=0.0,\n",
    "            top_p=0,\n",
    "            max_tokens=1,\n",
    "            stop=[\"###\"]\n",
    "        )\n",
    "    else:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"ada:ft-codewise-2023-03-10-04-22-36\",\n",
    "            prompt=q['background'] + \"\\n\\nDate of event: \" + q['publish_time'] + \"\\n\\nWhat is the answer to this question:\\n\" + q['question'] + \"\\n\\nDate of question: \" + q['publish_time'] + \"\\n\\nChoices: \" + str(q['choices']) + \"\\n\\n###\\n\\n\",\n",
    "            temperature=0.0,\n",
    "            top_p=0,\n",
    "            max_tokens=1,\n",
    "            stop=[\"###\"]\n",
    "        )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6532/6532 [09:56<00:00, 10.95it/s] \n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "responses = []\n",
    "# use tqdm to show progress\n",
    "for question in tqdm(autocast_questions):\n",
    "    response = fine_tuned_gpt(question)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the responses to a pickle file\n",
    "with open('./temp/gpt3_responses.pkl', 'wb') as f:\n",
    "    pickle.dump(responses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the responses from the pickle file\n",
    "with open('./temp/gpt3_responses.pkl', 'rb') as f:\n",
    "    responses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'A', 'yes', 'yes', 'no', 'yes', 'yes', 'no', 'yes', 'A', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'C', 'no', 'A', 'yes', 'A', 'B', 'yes', 'yes', 'yes', 'no', 'A', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'B', 'yes', 'C', 'yes', 'D', 'C', 'no', 'B', 'no', 'no', 'no', 'yes', 'D']\n",
      "['C', 'A', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'A', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'A', 'no', 'A', 'yes', 'A', 'B', 'yes', 'no', 'no', 'no', 'C', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'B', 'no', 'A', 'yes', 'A', 'C', 'no', 'B', 'no', 'no', 'no', 'no', 'D']\n",
      "Lengths:  6532 6532\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "correct = []\n",
    "for question in autocast_questions:\n",
    "    correct.append(question['answer'])\n",
    "    \n",
    "print(correct[:50])\n",
    "print(responses[:50])\n",
    "print(\"Lengths: \", len(correct), len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2797 2797 2797\n"
     ]
    }
   ],
   "source": [
    "for idx, question in enumerate(autocast_questions):\n",
    "    if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "        continue\n",
    "    if question['answer'] is None: # skipping questions without answer\n",
    "        continue\n",
    "    if question['qtype'] == 't/f':\n",
    "        ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "        pred_idx = 0 if responses[idx] == 'no' else 1\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        pred = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        pred[pred_idx] = 1\n",
    "        qtypes.append('t/f')\n",
    "    elif question['qtype'] == 'mc':\n",
    "        ans_idx = ord(question['answer']) - ord('A')\n",
    "        pred_idx = ord(responses[idx]) - ord('A')\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        pred = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        pred[pred_idx] = 1\n",
    "        qtypes.append('mc')\n",
    "    elif question['qtype'] == 'num':\n",
    "        # if response is not a number, skip the question\n",
    "        pred = float(responses[idx])\n",
    "        ans = float(question['answer'])\n",
    "        qtypes.append('num')\n",
    "    answers.append(ans)\n",
    "    preds.append(pred)\n",
    "print(len(answers), len(preds), len(qtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F: 16.16, MCQ: 41.67, NUM: 41.94\n",
      "Combined Metric: 99.77\n"
     ]
    }
   ],
   "source": [
    "tf_results, mc_results, num_results = [],[],[]\n",
    "for p, a, qtype in zip(preds, answers, qtypes):\n",
    "    if qtype == 't/f':\n",
    "        tf_results.append(brier_score(p, a))\n",
    "    elif qtype == 'mc':\n",
    "        mc_results.append(brier_score(p, a))\n",
    "    else:\n",
    "        num_results.append(np.abs(p - a))\n",
    "\n",
    "print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
